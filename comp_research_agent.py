# -*- coding: utf-8 -*-
"""Company_Research_Agent_Tavily_Llama3-8b-web.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1K5qkTfxaIU60R8GqIDXZuVtw9tP7uIP1
"""

# Install required packages
#!pip install transformers accelerate bitsandbytes tavily-python nest-asyncio beautifulsoup4

import os
import torch
import asyncio
import re
import argparse  # Added for command-line arguments
from getpass import getpass
from bs4 import BeautifulSoup
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
from transformers import BitsAndBytesConfig
from tavily import TavilyClient
import nest_asyncio

# Apply nest_asyncio for better async performance
nest_asyncio.apply()

# Configuration for 4-bit quantization
quant_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=True
)

# Load model with optimized settings
model_id = "McGill-NLP/Llama-3-8B-Web"
tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    quantization_config=quant_config,
    device_map="auto",
    torch_dtype=torch.float16,
    trust_remote_code=True
)

# Create optimized text-generation pipeline
pipe = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    max_new_tokens=574,        # Reduced for faster generation
    temperature=0.7,
    do_sample=True,
    num_return_sequences=1,
    pad_token_id=tokenizer.eos_token_id
)

# Get Tavily API key securely
TAVILY_API_KEY = "Enter your API Key"
tavily = TavilyClient(api_key=TAVILY_API_KEY)

# Define report sections
sections = [
    "Overview",
    "Financials",
    "Operations",
    "Market",
    "Digital",
    "Recent News",
    "Employer"
]

# Optimized prompts for search queries
query_prompts = {
    "Overview": "{company_name} founding date CEO headquarters",
    "Financials": "{company_name} revenue funding financial report",
    "Operations": "{company_name} employee count locations",
    "Market": "{company_name} competitors market share",
    "Digital": "{company_name} social media followers",
    "Recent News": "{company_name} news last week",
    "Employer": "{company_name} Glassdoor ratings jobs"
}

# Optimized section instructions
section_instructions = {
    "Overview": "Provide a 100-word overview of {company_name} including founding date, leadership, and key milestones. Cite sources with URLs and confidence scores (1-5). Complete the entire answer properly, do not add any html element into the answer",
    "Financials": "Provide a 100-word financial summary of {company_name} including revenue and funding. Cite sources with URLs and confidence scores (1-5).Complete the entire answer properly, do not add any html element into the answer",
    "Operations": "Describe {company_name}'s operations in 100 words including employee count and locations. Cite sources with URLs and confidence scores (1-5).Complete the entire answer properly, do not add any html element into the answer",
    "Market": "Analyze {company_name}'s market position in 100 words including competitors. Cite sources with URLs and confidence scores (1-5).Complete the entire answer properly, do not add any html element into the answer",
    "Digital": "Detail {company_name}'s digital presence in 100 words including social media stats. Cite sources with URLs and confidence scores (1-5).Complete the entire answer properly, do not add any html element into the answer",
    "Recent News": "Summarize recent news about {company_name} in 100 words. Cite sources with URLs and confidence scores (1-5).Complete the entire answer properly, do not add any html element into the answer",
    "Employer": "Evaluate {company_name} as an employer in 100 words. Cite sources with URLs and confidence scores (1-5).Complete the entire answer properly, do not add any html element into the answer"
}

# Function to clean HTML content
def clean_content(content):
    """Remove HTML tags and truncate content"""
    # Remove HTML tags
    clean_text = BeautifulSoup(content, "html.parser").get_text()
    # Remove excessive whitespace
    clean_text = re.sub(r'\s+', ' ', clean_text)
    # Remove AI system messages
    clean_text = re.sub(r'You are an AI assistant.*?;', '', clean_text, flags=re.DOTALL)
    # Truncate while preserving whole words
    return clean_text[:500] + '...' if len(clean_text) > 500 else clean_text

# Function to generate search query
def generate_query(section, company_name):
    return query_prompts[section].format(company_name=company_name)

# Async function for parallel searches
async def async_search(query, section):
    try:
        if section == "Recent News":
            return tavily.search(query=query, topic="news", days=7, max_results=3)
        return tavily.search(query=query, max_results=3)
    except Exception as e:
        print(f"Error searching {section}: {e}")
        return {"results": []}

# Async search manager
async def perform_searches(company_name):
    tasks = []
    for section in sections:
        query = generate_query(section, company_name)
        tasks.append(async_search(query, section))
    return await asyncio.gather(*tasks)

# Function to generate summary with token optimization
def generate_summary(section, company_name, search_results):
    if not search_results:
        return "No relevant information found", []

    # Format results efficiently
    formatted_results = "\n\n".join(
        f"Source {i+1}: {res['title']}\nURL: {res['url']}\nContent: {clean_content(res['content'])}"
        for i, res in enumerate(search_results)
    )

    # Create optimized prompt
    instructions = section_instructions[section].format(company_name=company_name)
    prompt = f"""
    [INSTRUCTION]
    {instructions}

    [SEARCH RESULTS]
    {formatted_results}

    [SUMMARY]
    """

    # Generate summary
    try:
        output = pipe(
            prompt,
            max_new_tokens=400,
            do_sample=True,
            temperature=0.7,
            top_p=0.9
        )[0]['generated_text']

        # Extract only the summary part
        summary = output.split("[SUMMARY]")[-1].strip()
        return summary, [res['url'] for res in search_results]
    except Exception as e:
        print(f"Error generating {section} summary: {e}")
        return "Summary generation failed", []

# Main function with performance improvements
def main(company_name):
    print(f"\nüîç Starting research on {company_name}...")

    # Run all searches in parallel
    print("‚ö° Conducting parallel searches...")
    search_responses = asyncio.run(perform_searches(company_name))

    report_sections = []
    all_sources = set()

    # Process each section
    for i, section in enumerate(sections):
        print(f"üìù Generating {section} section...")
        search_results = search_responses[i].get("results", [])
        summary, sources = generate_summary(section, company_name, search_results)
        report_sections.append((section, summary, sources))
        all_sources.update(sources)

    # Compile report
    report = f"# Comprehensive Report: {company_name}\n\n"
    for section, summary, sources in report_sections:
        report += f"## {section}\n{summary}\n\n"
        report += "### Sources\n" + "\n".join(f"- {url}" for url in sources) + "\n\n"
    
    # Add all sources section
    report += "## All Sources\n"
    for url in sorted(all_sources):
        report += f"- {url}\n"

    print("\n‚úÖ Report generated successfully!")
    return report  # Return report instead of printing directly

# Run with command-line argument
if __name__ == "__main__":
    # Setup command-line argument parser
    parser = argparse.ArgumentParser(description='Generate company research report.')
    parser.add_argument('--company', required=True, help='Company name to research')
    args = parser.parse_args()
    
    # Generate report
    report = main(args.company)
    
    # Print report to console
    print(f"\n=== Final Report ===\n{report}")
    
    # Save report to markdown file
    filename = f"{args.company.replace(' ', '_')}_report.md"
    with open(filename, 'w', encoding='utf-8') as f:
        f.write(report)
    print(f"\nüíæ Report saved to {filename}")